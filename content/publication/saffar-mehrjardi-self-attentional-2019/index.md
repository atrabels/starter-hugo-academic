---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems
subtitle: ''
summary: ''
authors:
- Mansour Saffar Mehrjardi
- Amine Trabelsi
- Osmar R. Za√Øane
tags: []
categories: []
date: '2019-09-01'
lastmod: 2021-09-06T12:29:39+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-09-06T11:29:39.508938Z'
publication_types:
- '1'
abstract: Self-attentional models are a new paradigm for sequence modelling tasks
  which differ from common sequence modelling methods, such as recurrence-based and
  convolution-based sequence learning, in the way that their architecture is only
  based on the attention mechanism. Self-attentional models have been used in the
  creation of the state-of-the-art models in many NLP task such as neural machine
  translation, but their usage has not been explored for the task of training end-to-end
  task-oriented dialogue generation systems yet. In this study, we apply these models
  on the DSTC2 dataset for training task-oriented chatbots. Our finding shows that
  self-attentional models can be exploited to create end-to-end task-oriented chatbots
  which not only achieve higher evaluation scores compared to recurrence-based models,
  but also do so more efficiently.
publication: '*Proceedings of the International Conference on Recent Advances in Natural
  Language Processing (RANLP 2019)*'
url_pdf: https://aclanthology.org/R19-1119
doi: 10.26615/978-954-452-056-4_119
---
